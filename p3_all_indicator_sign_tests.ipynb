{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9ce50e3-9328-498f-ad13-ae2a4f81eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from ghcn import load_daily\n",
    "from glob import glob\n",
    "import scipy.stats as stats\n",
    "\n",
    "from statsmodels.stats.descriptivestats import sign_test\n",
    "# Data files\n",
    "files = sorted(glob('ghcnd_small/*.dly'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "386cb41a-7ffb-4399-9170-626b7f9589f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data about stations (we only the station ID and latitude)\n",
    "# that are in the northern hemisphere (a.k.a. latitude > 0)\n",
    "north_stations = pd.read_fwf(\"ghcnd-stations.txt\", header=None, usecols=[0, 1])\n",
    "north_stations.columns = [\"station_id\", \"latitude\"]\n",
    "north_stations = north_stations[north_stations[\"latitude\"] > 0]\n",
    "north_stations.set_index(\"station_id\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18b8f7e-f444-4cab-ad5f-0a0fe4927b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:31<00:00, 10.96it/s]\n"
     ]
    }
   ],
   "source": [
    "data_all_t = []\n",
    "for filename in tqdm.tqdm(files):\n",
    "    # Get the station name from the filename\n",
    "    station_name = filename.split(sep='/')[-1][:-4]\n",
    "    # Get the latitude of the station if it's in the dictionary (if not, it's -1)\n",
    "    try:\n",
    "        latitude = north_stations['latitude'][station_name]\n",
    "    except:\n",
    "        latitude = -1\n",
    "    # Only load this file if the station is in the northern hemisphere (a.k.a. having latitude > 0)\n",
    "    if latitude < 0:\n",
    "        continue\n",
    "    \n",
    "    # All the data for one station\n",
    "    df_temp = pd.DataFrame.from_records(load_daily(filename))\n",
    "\n",
    "    # Extract the temperature data\n",
    "    filter_ = np.logical_or(df_temp[\"element\"] == \"TMIN\", df_temp[\"element\"] == \"TMAX\")\n",
    "    \n",
    "    temperatures = df_temp[filter_]\n",
    "\n",
    "    # Delete unnecessary columns\n",
    "    temperatures = temperatures.drop(columns=[\"measurement\", \"quality\", \"source\"])\n",
    "\n",
    "    data_all_t.append(temperatures)\n",
    "\n",
    "    del filename, station_name, latitude, df_temp, filter_, temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eac5f6a4-babf-4235-94ea-044dad78844f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>element</th>\n",
       "      <th>day</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AGM00060490</td>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AGM00060490</td>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>2</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AGM00060490</td>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AGM00060490</td>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>4</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AGM00060490</td>\n",
       "      <td>1957</td>\n",
       "      <td>1</td>\n",
       "      <td>TMAX</td>\n",
       "      <td>5</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50821</th>\n",
       "      <td>VMM00048808</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>13</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50822</th>\n",
       "      <td>VMM00048808</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>14</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50823</th>\n",
       "      <td>VMM00048808</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>15</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50824</th>\n",
       "      <td>VMM00048808</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>16</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50825</th>\n",
       "      <td>VMM00048808</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>TMIN</td>\n",
       "      <td>17</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7322806 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_id  year  month element  day  value\n",
       "0      AGM00060490  1957      1    TMAX    1    178\n",
       "1      AGM00060490  1957      1    TMAX    2    150\n",
       "2      AGM00060490  1957      1    TMAX    3    161\n",
       "3      AGM00060490  1957      1    TMAX    4    172\n",
       "4      AGM00060490  1957      1    TMAX    5    172\n",
       "...            ...   ...    ...     ...  ...    ...\n",
       "50821  VMM00048808  2021      3    TMIN   13    195\n",
       "50822  VMM00048808  2021      3    TMIN   14    196\n",
       "50823  VMM00048808  2021      3    TMIN   15    211\n",
       "50824  VMM00048808  2021      3    TMIN   16    212\n",
       "50825  VMM00048808  2021      3    TMIN   17    214\n",
       "\n",
       "[7322806 rows x 6 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all_t = pd.concat(data_all_t)\n",
    "\n",
    "# Delete missing data\n",
    "df_t = data_all_t[data_all_t[\"value\"] != -9999]\n",
    "df_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83bd0b57-d9f8-425d-a23f-f4a20b82957a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11486, 1)\n"
     ]
    }
   ],
   "source": [
    "# ===========================================================\n",
    "# Compute each station's annual mean temperature\n",
    "# ===========================================================\n",
    "\n",
    "# For each station and for each day, compute the midpoint temperature by\n",
    "# averaging the min and max temperatures\n",
    "mid_temps = df_t.where(np.logical_or(df_t[\"element\"] == \"TMIN\",\n",
    "                            df_t[\"element\"] == \"TMAX\")).groupby(by=[\"station_id\", \"year\", \"month\", \"day\"]).mean().reset_index()\n",
    "\n",
    "# For each station and for each year, compute the average temperature across that year\n",
    "temps = mid_temps.groupby(by=[\"station_id\", \"year\"]).mean()[\"value\"].reset_index()\n",
    "\n",
    "# All temperatures are in tenths of degree Celsius, so divide by 10 to get\n",
    "# actual Celsius temperatures\n",
    "temps[\"value\"] /= 10\n",
    "\n",
    "# Convert year to int type\n",
    "temps[\"year\"] = temps[\"year\"].astype(int)\n",
    "\n",
    "temps = temps.set_index([\"station_id\", \"year\"])\n",
    "temps.rename(columns={\"value\": \"temp\"}, inplace=True)\n",
    "print(temps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "27f77e21-8aee-4ad5-a144-d174616426fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AGM00060490</th>\n",
       "      <th>1957</th>\n",
       "      <td>17.622492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>18.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>18.911058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>19.716121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>20.243947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">VMM00048808</th>\n",
       "      <th>2017</th>\n",
       "      <td>20.512021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>21.082609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>23.860252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020</th>\n",
       "      <td>20.951423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>16.163077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11486 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       temp\n",
       "station_id  year           \n",
       "AGM00060490 1957  17.622492\n",
       "            1958  18.110000\n",
       "            1959  18.911058\n",
       "            1960  19.716121\n",
       "            1961  20.243947\n",
       "...                     ...\n",
       "VMM00048808 2017  20.512021\n",
       "            2018  21.082609\n",
       "            2019  23.860252\n",
       "            2020  20.951423\n",
       "            2021  16.163077\n",
       "\n",
       "[11486 rows x 1 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fff561b-2b96-4e82-9a72-98db2a8f7173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:53<00:00,  8.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files... DONE\n"
     ]
    }
   ],
   "source": [
    "# We only care about stations in the northern hemisphere (latitude > 0)\n",
    "# So go through the list of files, and only load files corresponding to\n",
    "# stations in the northern hemisphere.\n",
    "data_all = []\n",
    "for filename in tqdm.tqdm(files):\n",
    "    # Get the station name from the filename\n",
    "    station_name = filename.split(sep='/')[-1][:-4]\n",
    "    # Get the latitude of the station if it's in the dictionary (if not, it's -1)\n",
    "    try:\n",
    "        latitude = north_stations['latitude'][station_name]\n",
    "    except:\n",
    "        latitude = -1\n",
    "    # Only load this file if the station is in the northern hemisphere (a.k.a. having latitude > 0)\n",
    "    if latitude < 0:\n",
    "        continue\n",
    "    \n",
    "    # All the data for one station\n",
    "    df = pd.DataFrame.from_records(load_daily(filename))\n",
    "\n",
    "    # Extract the temperature data\n",
    "    filter_ = np.logical_and(np.logical_and(np.logical_and(np.logical_and(\n",
    "                                        df[\"element\"] != \"TMIN\",\n",
    "                                        df[\"element\"] != \"TMAX\"), \n",
    "                                        df[\"element\"] != \"PRCP\"), \n",
    "                                        df[\"element\"] != \"SNOW\"),\n",
    "                                        df[\"element\"] != \"TAVG\")\n",
    "    \n",
    "    temperatures = df[filter_]\n",
    "\n",
    "    # Delete unnecessary columns\n",
    "    temperatures = temperatures.drop(columns=[\"measurement\", \"quality\", \"source\"])\n",
    "\n",
    "    data_all.append(temperatures)\n",
    "\n",
    "    del filename, station_name, latitude, df, filter_, temperatures\n",
    "print(\"Reading files... DONE\")\n",
    "\n",
    "# Combine all the dataframes\n",
    "data_all = pd.concat(data_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d892ef1-8c58-44e7-8c5d-92fffdcb90df",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_data = data_all[data_all.value != -9999].dropna()\n",
    "df = nn_data[nn_data.element != 'SNWD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "94e82164-573e-4e8c-b54b-21e0091dcc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>element</th>\n",
       "      <th>day</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>ASN00001031</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>DAPR</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>805</th>\n",
       "      <td>ASN00001031</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>DWPR</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>ASN00001031</td>\n",
       "      <td>1998</td>\n",
       "      <td>10</td>\n",
       "      <td>MDPR</td>\n",
       "      <td>31</td>\n",
       "      <td>770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>ASN00001031</td>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>DAPR</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>ASN00001031</td>\n",
       "      <td>1998</td>\n",
       "      <td>11</td>\n",
       "      <td>DWPR</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95554</th>\n",
       "      <td>USW00094724</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>WSF5</td>\n",
       "      <td>13</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95555</th>\n",
       "      <td>USW00094724</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>WSF5</td>\n",
       "      <td>14</td>\n",
       "      <td>183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95556</th>\n",
       "      <td>USW00094724</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>WSF5</td>\n",
       "      <td>15</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95557</th>\n",
       "      <td>USW00094724</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>WSF5</td>\n",
       "      <td>16</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95558</th>\n",
       "      <td>USW00094724</td>\n",
       "      <td>2021</td>\n",
       "      <td>3</td>\n",
       "      <td>WSF5</td>\n",
       "      <td>17</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2474052 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        station_id  year  month element  day  value\n",
       "774    ASN00001031  1998     10    DAPR   31     16\n",
       "805    ASN00001031  1998     10    DWPR   31      5\n",
       "836    ASN00001031  1998     10    MDPR   31    770\n",
       "897    ASN00001031  1998     11    DAPR   30      4\n",
       "928    ASN00001031  1998     11    DWPR   30      4\n",
       "...            ...   ...    ...     ...  ...    ...\n",
       "95554  USW00094724  2021      3    WSF5   13    188\n",
       "95555  USW00094724  2021      3    WSF5   14    183\n",
       "95556  USW00094724  2021      3    WSF5   15    188\n",
       "95557  USW00094724  2021      3    WSF5   16     85\n",
       "95558  USW00094724  2021      3    WSF5   17     81\n",
       "\n",
       "[2474052 rows x 6 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240831d-4133-4343-a6c0-3a5bb301d48b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af2f56b6-6c37-4553-91d4-cbc73f9e17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_yearly_metrics(metrics):\n",
    "    metrics_data = []\n",
    "    for metric in tqdm.tqdm(metrics):\n",
    "        # Extract the data\n",
    "        data = df.where(df['element'] == metric).dropna()\n",
    "        \n",
    "        # Delete missing data, and unnecessary columns\n",
    "        data = data[data[\"value\"] != -9999]\n",
    "\n",
    "        # For each station and for each year, compute the average metric across that year\n",
    "        result = data.groupby(by=[\"station_id\", \"year\"]).mean()[\"value\"].reset_index()\n",
    "\n",
    "        # Convert year to int type\n",
    "        result[\"year\"] = result[\"year\"].astype(int)\n",
    "        \n",
    "        result = result.set_index([\"station_id\", \"year\"])\n",
    "        result.rename(columns={\"value\": metric}, inplace=True)\n",
    "        \n",
    "        metrics_data.append(result)\n",
    "\n",
    "    return metrics_data\n",
    "\n",
    "# Join the temperature and metric data (on the index a.k.a. the year column)\n",
    "metrics_list = df['element'].unique()\n",
    "\n",
    "# data = pd.concat(data, axis=1, join=\"inner\", ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5b08cd8-7663-489f-876d-19f073bc6d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 72/72 [01:27<00:00,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11486, 1), (3434, 1), (1282, 1), (3825, 1), (12, 1), (12, 1), (12, 1), (12, 1), (250, 1), (106, 1), (117, 1), (1052, 1), (776, 1), (1904, 1), (491, 1), (494, 1), (403, 1), (1141, 1), (378, 1), (2086, 1), (911, 1), (1342, 1), (190, 1), (4143, 1), (1007, 1), (184, 1), (256, 1), (241, 1), (107, 1), (107, 1), (52, 1), (52, 1), (350, 1), (109, 1), (11, 1), (100, 1), (101, 1), (43, 1), (43, 1), (61, 1), (61, 1), (9, 1), (9, 1), (10, 1), (10, 1), (145, 1), (13, 1), (185, 1), (71, 1), (132, 1), (131, 1), (132, 1), (131, 1), (41, 1), (2, 1), (44, 1), (35, 1), (1, 1), (1, 1), (36, 1), (37, 1), (7, 1), (6, 1), (6, 1), (33, 1), (47, 1), (17, 1), (6, 1), (36, 1), (4, 1), (6, 1), (28, 1), (28, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data = [temps] + compute_yearly_metrics(metrics_list)\n",
    "print([x.shape for x in data])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a218e3e0-3b69-46ed-8986-5044420785fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s_data = [x if x.shape[0] > 400 else _ for x in data]\n",
    "data_df = pd.concat(data, axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "35933d8d-ba1e-4628-b95a-8a89b33b9a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>temp</th>\n",
       "      <th>DAPR</th>\n",
       "      <th>DWPR</th>\n",
       "      <th>MDPR</th>\n",
       "      <th>DATN</th>\n",
       "      <th>MDTN</th>\n",
       "      <th>DATX</th>\n",
       "      <th>MDTX</th>\n",
       "      <th>MDSF</th>\n",
       "      <th>WDFG</th>\n",
       "      <th>...</th>\n",
       "      <th>WSFM</th>\n",
       "      <th>WT15</th>\n",
       "      <th>WT17</th>\n",
       "      <th>WT21</th>\n",
       "      <th>WV20</th>\n",
       "      <th>WT22</th>\n",
       "      <th>WV01</th>\n",
       "      <th>WV03</th>\n",
       "      <th>WDF1</th>\n",
       "      <th>WSF1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>station_id</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">AGM00060490</th>\n",
       "      <th>1957</th>\n",
       "      <td>17.622492</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>18.110000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>18.911058</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>19.716121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>20.243947</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">USC00410784</th>\n",
       "      <th>2020</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00047750</th>\n",
       "      <th>1945</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00226750</th>\n",
       "      <th>1950</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USC00238160</th>\n",
       "      <th>1948</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15601 rows × 73 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       temp  DAPR  DWPR  MDPR  DATN  MDTN  DATX  MDTX  MDSF  \\\n",
       "station_id  year                                                              \n",
       "AGM00060490 1957  17.622492   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1958  18.110000   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1959  18.911058   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1960  19.716121   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1961  20.243947   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...                     ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "USC00410784 2020        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            2021        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00047750 1945        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00226750 1950        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00238160 1948        NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "                  WDFG  ...  WSFM  WT15  WT17  WT21  WV20  WT22  WV01  WV03  \\\n",
       "station_id  year        ...                                                   \n",
       "AGM00060490 1957   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1958   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1959   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1960   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            1961   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "...                ...  ...   ...   ...   ...   ...   ...   ...   ...   ...   \n",
       "USC00410784 2020   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "            2021   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00047750 1945   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00226750 1950   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "USC00238160 1948   NaN  ...   NaN   NaN   NaN   NaN   NaN   NaN   NaN   NaN   \n",
       "\n",
       "                  WDF1  WSF1  \n",
       "station_id  year              \n",
       "AGM00060490 1957   NaN   NaN  \n",
       "            1958   NaN   NaN  \n",
       "            1959   NaN   NaN  \n",
       "            1960   NaN   NaN  \n",
       "            1961   NaN   NaN  \n",
       "...                ...   ...  \n",
       "USC00410784 2020   NaN   NaN  \n",
       "            2021   NaN   NaN  \n",
       "USC00047750 1945   NaN   NaN  \n",
       "USC00226750 1950   NaN   NaN  \n",
       "USC00238160 1948   NaN   NaN  \n",
       "\n",
       "[15601 rows x 73 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50688fb8-cf6c-493e-a391-0733567a9a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Once all the data for each station and year is calculated, \n",
    "build a factor vs temp list for all factors and make a sliding \n",
    "window of adjacent values per year.\n",
    "'''\n",
    "\n",
    "\n",
    "factor_df_list = []\n",
    "for col in data_df.columns:\n",
    "    if col == 'temp':\n",
    "        continue\n",
    "    factor_df_list.append(data_df[['temp', col]])\n",
    "        \n",
    "factor_df_list_adj = []\n",
    "for df_ in factor_df_list:\n",
    "    tr_df = df_.dropna().reset_index(level=\"year\")\n",
    "    year_column = tr_df[\"year\"]\n",
    "    all_years = year_column.unique()\n",
    "    all_years.sort()\n",
    "    if len(all_years) <= 2:\n",
    "        continue\n",
    "    all_adj_years = np.lib.stride_tricks.sliding_window_view(all_years, 2)\n",
    "    factor_df_list_adj.append((tr_df, all_adj_years)) if df_.dropna().shape[0] > 100 else _\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97330a29-3163-4519-9bbe-e1be02935d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# factor_df_list_adj[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "68225369-eabe-4b92-b8aa-deb21b9f8f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 96/96 [00:00<00:00, 158.43it/s]\n",
      "100%|██████████| 105/105 [00:00<00:00, 280.32it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 373.08it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 361.08it/s]\n",
      "100%|██████████| 52/52 [00:00<00:00, 325.87it/s]\n",
      "100%|██████████| 57/57 [00:00<00:00, 140.89it/s]\n",
      "100%|██████████| 125/125 [00:00<00:00, 149.98it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 248.42it/s]\n",
      "100%|██████████| 113/113 [00:00<00:00, 327.49it/s]\n",
      "100%|██████████| 110/110 [00:00<00:00, 300.87it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 301.87it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 299.15it/s]\n",
      "100%|██████████| 128/128 [00:00<00:00, 260.31it/s]\n",
      "100%|██████████| 96/96 [00:00<00:00, 252.95it/s]\n",
      "100%|██████████| 126/126 [00:00<00:00, 242.79it/s]\n",
      "100%|██████████| 69/69 [00:00<00:00, 384.02it/s]\n",
      "100%|██████████| 124/124 [00:00<00:00, 199.13it/s]\n",
      "100%|██████████| 111/111 [00:00<00:00, 225.46it/s]\n",
      "100%|██████████| 73/73 [00:00<00:00, 304.91it/s]\n",
      "100%|██████████| 82/82 [00:00<00:00, 325.60it/s]\n",
      "100%|██████████| 81/81 [00:00<00:00, 268.60it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 294.33it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 324.78it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 373.68it/s]\n",
      "100%|██████████| 37/37 [00:00<00:00, 221.98it/s]\n",
      "100%|██████████| 56/56 [00:00<00:00, 317.31it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 253.51it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 345.13it/s]\n",
      "100%|██████████| 25/25 [00:00<00:00, 259.09it/s]\n",
      "100%|██████████| 24/24 [00:00<00:00, 220.65it/s]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "With a alpha of 0.05, calculate sign tests for each of the \n",
    "climate indicators for each pair of adjacent years. Calculate the proportion \n",
    "of years which were significant.\n",
    "'''\n",
    "\n",
    "alpha = 0.05\n",
    "\n",
    "outputs = []\n",
    "for df_adj in factor_df_list_adj:\n",
    "    df_, all_adj_years = df_adj\n",
    "    num_sig_factor = 0\n",
    "    num_tests = len(all_adj_years)\n",
    "    \n",
    "    for (year1, year2) in tqdm.tqdm(all_adj_years):\n",
    "        # Get station data for both years\n",
    "        data_year1 = df_[df_['year'] == year1]\n",
    "        data_year2 = df_[df_['year'] == year2]\n",
    "        \n",
    "        # Get specific columns\n",
    "        temp_year1 = data_year1[\"temp\"]\n",
    "        temp_year2 = data_year2[\"temp\"]\n",
    "        factor_year1 = data_year1[data_year1.columns[-1]]\n",
    "        factor_year2 = data_year2[data_year2.columns[-1]]\n",
    "\n",
    "        # Inner join on station id to find observations common between both years, and\n",
    "        # then compute the differences\n",
    "        temp_year_diff = pd.DataFrame(temp_year1).join(temp_year2, how=\"inner\",\n",
    "                                                         lsuffix=\"_{}\".format(year1),\n",
    "                                                         rsuffix=\"_{}\".format(year2)).diff(axis=1).iloc[:, 1]\n",
    "        factor_year_diff = pd.DataFrame(factor_year1).join(factor_year2, how=\"inner\",\n",
    "                                                         lsuffix=\"_{}\".format(year1),\n",
    "                                                         rsuffix=\"_{}\".format(year2)).diff(axis=1).iloc[:, 1]    \n",
    "        # Only looking at cases where there are at least 5 values. \n",
    "        # The sign test is able to handle a small number of values \n",
    "        # but we want to cap at 5 values minimum.\n",
    "        if factor_year_diff.shape[0] < 5 or temp_year_diff.shape[0] < 5:\n",
    "            continue\n",
    "        # Count the number of differences that are positive\n",
    "        #\n",
    "        # These three values will serve as our test statistics for a sign test on temperature,\n",
    "        # a sign test on precipitation, and a sign test on snowfall, respectively\n",
    "        test_stat_temp = np.count_nonzero(temp_year_diff > 0)\n",
    "        test_stat_factor = np.count_nonzero(factor_year_diff > 0)\n",
    "\n",
    "        # Convert each test statistic into a p-value using the binomial test. In this case, each\n",
    "        # test statistic is defined as the number of successes (a.k.a. the number of stations\n",
    "        # for which temperature increased, precipitation increased, and snowfall increased,\n",
    "        # respectively) out of all stations\n",
    "        p_value_temp = stats.binomtest(k=test_stat_temp, n=temp_year_diff.size, alternative='two-sided').pvalue\n",
    "        p_value_factor = stats.binomtest(k=test_stat_factor, n=factor_year_diff.size, alternative='two-sided').pvalue\n",
    "\n",
    "        # If the p-values for temperature and precipitation are BOTH significant, then it means\n",
    "        # that between year1 and year2, temperature and precipitation BOTH had a statistically\n",
    "        # significant increase\n",
    "        #\n",
    "        # UPDATE: We need to apply the Bonferroni correction, since we are testing multiple hypotheses on the same dataset \n",
    "        if p_value_temp < alpha/num_tests and p_value_factor < alpha/num_tests:\n",
    "            num_sig_factor += 1\n",
    "    outputs.append((df_.columns[-1], num_sig_factor/len(all_adj_years)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52365d60-c38c-48b9-b601-c30b4d4674fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WT01', 0.008), ('WT03', 0.03125), ('TOBS', 0.07258064516129033)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nonzero_outputs = []\n",
    "for op in outputs:\n",
    "    if op[1] != 0:\n",
    "        nonzero_outputs.append(op)\n",
    "nonzero_outputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c4dc74e0-1bff-482a-a96e-c5fc1da2a34f",
   "metadata": {},
   "source": [
    "#### Climate Indicator Significant proportions of adjacent years\n",
    " **WESD** (Water equivalent of snow on the ground) = 0.07017543859649122\n",
    " \n",
    " **WT01** (Fog, ice fog, or freezing fog (may include heavy fog)) = 0.184\n",
    " \n",
    " **WT08** (Smoke or haze) = 0.00847457627118644\n",
    " \n",
    " **WT16** (Rain) = 0.008849557522123894\n",
    " \n",
    " **WT18** (Snow, snow pellets, snow grains, or ice crystals) = 0.00909090909090909\n",
    " \n",
    " **WT05** (Hail) = 0.015873015873015872\n",
    " \n",
    " **WT09** (Blowing or drifting snow) = 0.02\n",
    " \n",
    " **WT03** (Thunder) = 0.1640625\n",
    " \n",
    " **WT06** (Glaze or rime) = 0.09375\n",
    " \n",
    " **WT04** (Ice pellets, sleet, snow pellets, or small hail) = 0.03968253968253968\n",
    " \n",
    " **TOBS** (Temperature at the time of observation) = 0.28225806451612906\n",
    " \n",
    " **WT11** (High or damaging winds) = 0.02702702702702703\n",
    " \n",
    "#### Analysis\n",
    " Our outputs here show a few different climate indicators and the respective significant proportions. These proportions show the fraction of yearly changes which had a significant change in temperature and in the respective indicator. We  ignore 'TOBS' as it refers to temperature at time of observation, which is a redundant indicator.\n",
    " \n",
    " For example, we look at 'WT01', which refers to Fog, ice fog, or freezing fog (may include heavy fog) climate patterns. Across all years, about 18% had significant increases in temperature and significant change in the weather pattern. This is valuable in telling us when increases in temperature affect the climate through different values. \n",
    " \n",
    " Another perspective would be that the increase in temperature and the increase in the climate indicators are not a causal effect, but have confounding variables in the mix which may be the cause for both. For example, WT03, Thunder, has a 16% significant factor. However we usually would not relate Thunder to increasing heat. Instead, there may be high humidity or hot weather near the respective stations which correlates with humid thunderstorms. \n",
    " \n",
    " What we find is that a majority of these indicators have confounding variables and are not directly related to temperature(i.e. Thunder or Fog). However, some indicators like WT04-6(Sleet/Ice Pellets, Hail, Glaze/Rime) are all very directly related to temperature. In general, higher temperatures equal less snow or ice. We cannot say the same for Thunder or WT08(Smoke or Haze)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
